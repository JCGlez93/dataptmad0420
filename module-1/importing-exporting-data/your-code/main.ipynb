{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before your start:\n",
    "- Read the README.md file\n",
    "- Comment as much as you can and use the resources in the README.md file\n",
    "- Happy learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buenas tardes,\n",
    "Existen un par de erratas en el lab [importing-exporting-data]. A continuación, os indico las instrucciones para poder realizar los Challenges 1 y 3 (en el lab se repite el nombre Challenge 2, por lo tanto me refiero al tercer challenge).\n",
    "Challenge 1. Por favor, descargaros desde el siguiente enlace el fichero .db y ejecutar las operaciones que se indican, es decir, conectarse a la base de datos desde Jupyter y realizar al menos una operación de JOIN entre las tablas (este fichero es un fichero diferente pero que igualmente contiene varias tablas).\n",
    "Challenge 3. En este caso el fichero al que se hace referencia tampoco se encuentra dentro de la carpeta data, por lo que podréis descargar en este enlace un fichero .zip para hacer las operaciones que se indican en el challenge, es decir, importar con pandas el fichero, en este caso .txt , y colocar el nombre a las columnas. Solo hay una variante y es que 23 de los 24 nombres de las columnas tenéis que extraerlos del otro fichero .csv que se encuentra dentro del mismo challenge3.zip\n",
    "Podéis dejar estos nuevos ficheros de datos en la carpeta data del lab.\n",
    "Cualquier duda nos comentáis!!!\n",
    ":marca_de_verificación_gruesa:\n",
    "21\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4 respuestas\n",
    "La última respuesta se publicó hoy a las 18:28Ver hilo\n",
    "\n",
    "Octavio Garcia Moreno  18:20\n",
    "ha respondido a un hilo:\n",
    "Buenas tardes,…\n",
    "Hola Ivan, toda la razón!\n",
    "Considerad por tanto mi propuesta para el Challenge 3 como un Bonus!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1 - Accessing our database\n",
    "Create a connection to access the sakila database with the ip, user and password provided in class. Take a look at the data. Do some joins in order to have more elaborated tables in a dataframe (you can try first the query in the DBMS and then use it in your notebook).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnx = sqlite3.connect('/home/jc/Escritorio/Github_labs/dataptmad0420/module-1/importing-exporting-data/data/juliarochflores.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8760,\n",
       "  'BEZOS',\n",
       "  '54 years old',\n",
       "  'United States',\n",
       "  'Technology  ==> Amazon',\n",
       "  '112.0 BUSD'),\n",
       " (7500,\n",
       "  'GATES',\n",
       "  '62 years old',\n",
       "  'None',\n",
       "  'Technology  ==> Microsoft',\n",
       "  '90.0 BUSD'),\n",
       " (5751,\n",
       "  'bufFETT',\n",
       "  '87 years old',\n",
       "  'None',\n",
       "  'Finance and Investments  ==> Berkshire Hathaway',\n",
       "  '84.0 BUSD'),\n",
       " (8365,\n",
       "  'arNAULt',\n",
       "  '69 years old',\n",
       "  'France',\n",
       "  'Fashion & Retail  ==> LVMH',\n",
       "  '72.0 BUSD'),\n",
       " (5328,\n",
       "  'zUCKERBERg',\n",
       "  '1985',\n",
       "  'United States',\n",
       "  'Technology  ==> Facebook',\n",
       "  '71.0 BUSD'),\n",
       " (4945,\n",
       "  'ortEGa',\n",
       "  '82 years old',\n",
       "  'Spain',\n",
       "  'Fashion & Retail  ==> Zara',\n",
       "  '70.0 BUSD'),\n",
       " (3712,\n",
       "  'slIM HELU',\n",
       "  '78 years old',\n",
       "  'None',\n",
       "  'Telecom  ==> telecom',\n",
       "  '67.1 BUSD'),\n",
       " (5137,\n",
       "  'KOCH',\n",
       "  '82 years old',\n",
       "  'None',\n",
       "  'Diversified  ==> Koch Industries',\n",
       "  '60.0 BUSD'),\n",
       " (1485,\n",
       "  'kOCH',\n",
       "  '78 years old',\n",
       "  'None',\n",
       "  'Diversified  ==> Koch Industries',\n",
       "  '60.0 BUSD'),\n",
       " (1108,\n",
       "  'eLLISOn',\n",
       "  '73 years old',\n",
       "  'United States',\n",
       "  'Technology  ==> software',\n",
       "  '58.5 BUSD')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnx.execute(\"SELECT personal_info.id , lastName as Last_Name , age , country ,Source , worth \\\n",
    "cnx.execute(\"SELECT personal_info.id , lastName as Last_Name , age , country ,Source , worth \\\n",
    "From personal_info LEFT Join business_info on personal_info.id = business_info.id LIMIT 10;\").fetchall()From personal_info LEFT Join business_info on personal_info.id = business_info.id LIMIT 10;\").fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-b19ac2b42b29>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-b19ac2b42b29>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    cnx.execute(\"SELECT personal_info.id , name ,  personal_info.\"Unnamed: 0\" as Ranking , lastName , age , gender , country , worth FROM rank_info  Join personal_info on personal_info.id = rank_info.id Join business_info on personal_info.id = business_info.id LIMIT 10;\").fetchall()\u001b[0m\n\u001b[0m                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "cnx.execute(\"SELECT personal_info.id , name ,  personal_info.\"Unnamed: 0\" as Ranking , lastName , age , gender , country , worth FROM rank_info  Join personal_info on personal_info.id = rank_info.id Join business_info on personal_info.id = business_info.id LIMIT 10;\").fetchall()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create table if not exists \"10Most_valued_companies\"  as \n",
    "SELECT personal_info.id , lastName as Last_Name , age , country ,Source , worth \n",
    "From personal_info\n",
    "LEFT Join business_info on \n",
    "personal_info.id = business_info.id \n",
    "LIMIT 10;\n",
    "\n",
    "\n",
    "Create table if not exists \"10Most_wealthy_people\"  as \n",
    "SELECT personal_info.id , name ,  personal_info.\"Unnamed: 0\" as Ranking , lastName , age , gender , country , worth\n",
    "FROM rank_info  \n",
    "Join personal_info on \n",
    "personal_info.id = rank_info.id \n",
    "JOIN business_info on \n",
    "personal_info.id = business_info.id\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Working with JSON files\n",
    "\n",
    "Import the pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, load the data in `nasa.json` in the `data` folder and load it into a pandas dataframe. Name the dataframe `nasa`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "nasa = pd.read_json('/home/jc/Escritorio/Github_labs/dataptmad0420/module-1/importing-exporting-data/data/nasa.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded the data, let's examine it using the `head()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>:@computed_region_cbhk_fwbd</th>\n",
       "      <th>:@computed_region_nnqa_25f4</th>\n",
       "      <th>fall</th>\n",
       "      <th>geolocation</th>\n",
       "      <th>id</th>\n",
       "      <th>mass</th>\n",
       "      <th>name</th>\n",
       "      <th>nametype</th>\n",
       "      <th>recclass</th>\n",
       "      <th>reclat</th>\n",
       "      <th>reclong</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fell</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [6.08333, 50....</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Aachen</td>\n",
       "      <td>Valid</td>\n",
       "      <td>L5</td>\n",
       "      <td>50.77500</td>\n",
       "      <td>6.08333</td>\n",
       "      <td>1880-01-01T00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fell</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [10.23333, 56...</td>\n",
       "      <td>2</td>\n",
       "      <td>720.0</td>\n",
       "      <td>Aarhus</td>\n",
       "      <td>Valid</td>\n",
       "      <td>H6</td>\n",
       "      <td>56.18333</td>\n",
       "      <td>10.23333</td>\n",
       "      <td>1951-01-01T00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fell</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-113, 54.216...</td>\n",
       "      <td>6</td>\n",
       "      <td>107000.0</td>\n",
       "      <td>Abee</td>\n",
       "      <td>Valid</td>\n",
       "      <td>EH4</td>\n",
       "      <td>54.21667</td>\n",
       "      <td>-113.00000</td>\n",
       "      <td>1952-01-01T00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fell</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-99.9, 16.88...</td>\n",
       "      <td>10</td>\n",
       "      <td>1914.0</td>\n",
       "      <td>Acapulco</td>\n",
       "      <td>Valid</td>\n",
       "      <td>Acapulcoite</td>\n",
       "      <td>16.88333</td>\n",
       "      <td>-99.90000</td>\n",
       "      <td>1976-01-01T00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fell</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-64.95, -33....</td>\n",
       "      <td>370</td>\n",
       "      <td>780.0</td>\n",
       "      <td>Achiras</td>\n",
       "      <td>Valid</td>\n",
       "      <td>L6</td>\n",
       "      <td>-33.16667</td>\n",
       "      <td>-64.95000</td>\n",
       "      <td>1902-01-01T00:00:00.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   :@computed_region_cbhk_fwbd  :@computed_region_nnqa_25f4  fall  \\\n",
       "0                          NaN                          NaN  Fell   \n",
       "1                          NaN                          NaN  Fell   \n",
       "2                          NaN                          NaN  Fell   \n",
       "3                          NaN                          NaN  Fell   \n",
       "4                          NaN                          NaN  Fell   \n",
       "\n",
       "                                         geolocation   id      mass      name  \\\n",
       "0  {'type': 'Point', 'coordinates': [6.08333, 50....    1      21.0    Aachen   \n",
       "1  {'type': 'Point', 'coordinates': [10.23333, 56...    2     720.0    Aarhus   \n",
       "2  {'type': 'Point', 'coordinates': [-113, 54.216...    6  107000.0      Abee   \n",
       "3  {'type': 'Point', 'coordinates': [-99.9, 16.88...   10    1914.0  Acapulco   \n",
       "4  {'type': 'Point', 'coordinates': [-64.95, -33....  370     780.0   Achiras   \n",
       "\n",
       "  nametype     recclass    reclat    reclong                     year  \n",
       "0    Valid           L5  50.77500    6.08333  1880-01-01T00:00:00.000  \n",
       "1    Valid           H6  56.18333   10.23333  1951-01-01T00:00:00.000  \n",
       "2    Valid          EH4  54.21667 -113.00000  1952-01-01T00:00:00.000  \n",
       "3    Valid  Acapulcoite  16.88333  -99.90000  1976-01-01T00:00:00.000  \n",
       "4    Valid           L6 -33.16667  -64.95000  1902-01-01T00:00:00.000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "nasa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `value_counts()` function is commonly used in pandas to find the frequency of every value in a column.\n",
    "\n",
    "In the cell below, use the `value_counts()` function to determine the frequency of all types of asteroid landings by applying the function to the `fall` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.value_counts of 0      Fell\n",
       "1      Fell\n",
       "2      Fell\n",
       "3      Fell\n",
       "4      Fell\n",
       "       ... \n",
       "995    Fell\n",
       "996    Fell\n",
       "997    Fell\n",
       "998    Fell\n",
       "999    Fell\n",
       "Name: fall, Length: 1000, dtype: object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "nasa['fall'].value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's save the dataframe as a json file again. Save the dataframe using the `orient=records` argument and name the file `nasa-output.json`. Remember to save the file inside the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "nasa.to_json('/home/jc/Escritorio/Github_labs/dataptmad0420/module-1/importing-exporting-data/data//nasa-output.json', orient='records')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Working with CSV and Other Separated Files\n",
    "\n",
    "CSV files are more commonly used as dataframes. In the cell below, load the file from the URL provided using the `read_csv()` function in pandas. Starting version 0.19 of pandas, you can load a CSV file into a dataframe directly from a URL without having to load the file first and then transform it. The dataset we will be using contains information about NASA shuttles.\n",
    "\n",
    "In the cell below, we define the column names and the URL of the data. Following this cell, read the tst file to a variable called `shuttle`. Since the file does not contain the column names, you must add them yourself using the column names declared in `cols` using the `names` argument. Additionally, a tst file is space separated, make sure you pass ` sep=' '` to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['time', 'rad_flow', 'fpv_close', 'fpv_open', 'high', 'bypass', 'bpv_close', 'bpv_open', 'class']\n",
    "tst_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/shuttle/shuttle.tst'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 5, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-4da4f4ed042c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mzf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/jc/Escritorio/Github_labs/dataptmad0420/module-1/importing-exporting-data/data/challenge3.zip'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# having First.csv zipped file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Food_Supply_Quantity_kg_Data_No_Header.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2035\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2037\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2038\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 5, saw 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "zf = zipfile.ZipFile('/home/jc/Escritorio/Github_labs/dataptmad0420/module-1/importing-exporting-data/data/challenge3.zip') # having First.csv zipped file.\n",
    "df = pd.read_csv(zf.open('Food_Supply_Quantity_kg_Data_No_Header.txt'), sep=\" \", header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categories</th>\n",
       "      <th>Items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alcoholic Beverages</td>\n",
       "      <td>Alcohol, Non-Food; Beer; Beverages, Alcoholic;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Animal fats</td>\n",
       "      <td>Butter, Ghee; Cream; Fats, Animals, Raw; Fish,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Animal Products</td>\n",
       "      <td>Aquatic Animals, Others; Aquatic Plants; Bovin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aquatic Products, Other</td>\n",
       "      <td>Aquatic Animals, Others; Aquatic Plants; Meat,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cereals - Excluding Beer</td>\n",
       "      <td>Barley and products; Cereals, Other; Maize and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Categories                                              Items\n",
       "0       Alcoholic Beverages  Alcohol, Non-Food; Beer; Beverages, Alcoholic;...\n",
       "1               Animal fats  Butter, Ghee; Cream; Fats, Animals, Raw; Fish,...\n",
       "2           Animal Products  Aquatic Animals, Others; Aquatic Plants; Bovin...\n",
       "3   Aquatic Products, Other  Aquatic Animals, Others; Aquatic Plants; Meat,...\n",
       "4  Cereals - Excluding Beer  Barley and products; Cereals, Other; Maize and..."
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that this worked by looking at the `head()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make life easier for us, let's turn this dataframe into a comma separated file by saving it using the `to_csv()` function. Save `shuttle` into the file `shuttle.csv` and ensure the file is comma separated, that we are not saving the index column and that the file is saved inside the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Working with Excel Files\n",
    "\n",
    "We can also use pandas to convert excel spreadsheets to dataframes. Let's use the `read_excel()` function. In this case, `astronauts.xls` is in the `data` folder. Read this file into a variable called `astronaut`. \n",
    "\n",
    "Note: Make sure to install the `xlrd` library if it is not yet installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `head()` function to inspect the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `value_counts()` function to find the most popular undergraduate major among all astronauts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to all the commas present in the cells of this file, let's save it as a tab separated csv file. In the cell below, save `astronaut` as a **tab separated file** using the `to_csv` function. Call the file `astronaut.csv`. Remember to remove the index column and save the file in the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge - Fertility Dataset\n",
    "\n",
    "Visit the following [URL](https://archive.ics.uci.edu/ml/datasets/Fertility) and retrieve the dataset as well as the column headers. Determine the correct separator and read the file into a variable called `fertility`. Examine the dataframe using the `head()` function. \n",
    "\n",
    "Look in Google for a way to retrieve this data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupyter_env] *",
   "language": "python",
   "name": "conda-env-jupyter_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
